# ğŸ“„ Smart Contract Q&A Assistant

AI-powered document analysis tool. Upload contracts, ask questions, get instant answers with citations - 100% local and private.

## âœ¨ Features

- ğŸ¤– Natural language Q&A on documents
- ğŸ“š Source citations for every answer
- ğŸ”’ 100% private - all processing local
- ğŸ“„ Supports PDF and DOCX files
- âš¡ Fast setup (5 minutes)
- ğŸ’¾ No API costs - uses free local LLM

## ğŸš€ Quick Start

### Prerequisites

```bash
# 1. Install Ollama (free local LLM)
# Download from: https://ollama.ai

# 2. Pull the model
ollama pull llama3.2
```

### Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Run the application
python main.py
```

**Open http://localhost:7864 in your browser!**

## ğŸ“– How to Use

### 1. Upload Document
- Click **"ğŸ“¤ Upload Document"** tab
- Select PDF or DOCX file
- Wait for processing (~10 seconds)

### 2. Ask Questions
- Switch to **"ğŸ’¬ Chat"** tab
- Type your question
- Get instant answers with citations

### 3. Get Summary (Optional)
- Click **"ğŸ“„ Summarize"** button

**Example Questions:**
- "What are the payment terms?"
- "Who are the parties involved?"
- "What is the termination clause?"

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Gradio    â”‚â”€â”€â”€â”€â–¶â”‚   FastAPI    â”‚â”€â”€â”€â”€â–¶â”‚   Ollama    â”‚
â”‚     UI      â”‚     â”‚   Backend    â”‚     â”‚  (LLM)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  ChromaDB    â”‚
                    â”‚ Vector Store â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tech Stack:**
- LangChain - RAG pipeline
- Ollama - Local LLM (free)
- ChromaDB - Vector database
- FastAPI - API backend
- Gradio - Web interface

## ğŸ“ Project Structure

```
smart-contract-assistant/
â”œâ”€â”€ main.py                 # Entry point - run this
â”œâ”€â”€ api_server.py          # FastAPI backend
â”œâ”€â”€ gradio_ui.py           # Web interface
â”œâ”€â”€ rag_chain.py           # RAG pipeline logic
â”œâ”€â”€ document_processor.py  # PDF/DOCX parsing
â”œâ”€â”€ vector_store.py        # Embedding management
â”œâ”€â”€ guardrails.py          # Answer validation
â”œâ”€â”€ config.py              # Configuration
â””â”€â”€ requirements.txt       # Dependencies
```

## âš™ï¸ Configuration

Edit `config.py`:

```python
LLM_MODEL = "llama3.2"        # Change model
CHUNK_SIZE = 1000             # Text chunk size
CHUNK_OVERLAP = 200           # Overlap
TOP_K = 5                     # Retrieved chunks
API_PORT = 8001               # Backend port
UI_PORT = 7864                # Frontend port
```

## ğŸ”Œ API Endpoints (Optional)

```bash
# Health check
GET http://localhost:8001/health

# Upload document
POST http://localhost:8001/upload

# Ask question
POST http://localhost:8001/qa?question=YOUR_QUESTION

# Get summary
POST http://localhost:8001/summarize
```

Interactive docs: **http://localhost:8001/docs** (when running)

## ğŸ› ï¸ Advanced Usage

### Run Components Separately

```bash
# Terminal 1: Start API
python api_server.py

# Terminal 2: Start UI
python gradio_ui.py
```

### API-Only Mode

```bash
python main.py --mode api
```

### UI-Only Mode

```bash
python main.py --mode ui
```

### Use Different Model

```python
# In config.py
LLM_MODEL = "mistral"  # or phi3, gemma, etc.
```

Then pull the model:
```bash
ollama pull mistral
```

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| "Ollama not found" | Install from [ollama.ai](https://ollama.ai) and run `ollama pull llama3.2` |
| "Port in use" | Change `API_PORT` or `UI_PORT` in config.py |
| Slow responses | Normal for local AI (3-8 sec), try smaller model |
| No answers | Verify Ollama is running: `ollama serve` |
| Module errors | Run `pip install -r requirements.txt` |

## ğŸ“Š Performance

- **Upload Time:** 5-10 seconds (avg 10-page PDF)
- **Query Response:** 3-8 seconds
- **Accuracy:** Grounded in source documents only
- **Privacy:** 100% local - no external API calls

## ğŸ“ What This Demonstrates

- âœ… RAG (Retrieval Augmented Generation) pipeline
- âœ… Vector database integration
- âœ… LLM orchestration with LangChain
- âœ… Microservices architecture (FastAPI)
- âœ… Document processing strategies
- âœ… Answer validation and guardrails

## ğŸ“„ License

MIT License - see LICENSE file

## ğŸ™ Built With

- [LangChain](https://www.langchain.com/)
- [Ollama](https://ollama.ai/)
- [ChromaDB](https://www.trychroma.com/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [Gradio](https://www.gradio.app/)

---

**Built for NVIDIA DLI Workshop on LLM Pipelines**
